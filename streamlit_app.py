# -*- coding: utf-8 -*-
"""streamlit_app

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15j8YA1XFrjpcYGl_v1R9bWpkGtSV-VC3
"""

# streamlit_app.py - Streamlit ëŒ€ì‹œë³´ë“œ

import streamlit as st
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import time
import re
from typing import Dict, Tuple

# ========================================
# í˜ì´ì§€ ì„¤ì •
# ========================================

st.set_page_config(
    page_title="EXAONE 1.2B LLM ëŒ€ì‹œë³´ë“œ",
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ì»¤ìŠ¤í…€ CSS
st.markdown("""
<style>
    .main {
        background: linear-gradient(135deg, #0f172a 0%, #1e293b 100%);
    }
    .stButton > button {
        background: linear-gradient(135deg, #38bdf8 0%, #0284c7 100%);
        color: white;
        border: none;
        border-radius: 8px;
        font-weight: 600;
        padding: 10px 24px;
    }
    .stButton > button:hover {
        background: linear-gradient(135deg, #0284c7 0%, #0166aa 100%);
    }
    .message-user {
        background: #38bdf8;
        color: #0f172a;
        padding: 12px 16px;
        border-radius: 8px;
        margin: 8px 0;
        max-width: 70%;
        margin-left: auto;
    }
    .message-assistant {
        background: rgba(56, 189, 248, 0.15);
        color: #cbd5e1;
        border-left: 3px solid #38bdf8;
        padding: 12px 16px;
        border-radius: 8px;
        margin: 8px 0;
        max-width: 85%;
    }
</style>
""", unsafe_allow_html=True)

# ========================================
# 1. ëª¨ë¸ ë¡œë“œ (ìºì‹±)
# ========================================

@st.cache_resource
def load_model():
    """ëª¨ë¸ ë¡œë“œ (í•œ ë²ˆë§Œ ì‹¤í–‰)"""

    MODEL_PATH = "/content/drive/MyDrive/exaone4_gsm8k2"
    BASE_MODEL = "LGAI-EXAONE/EXAONE-4.0-1.2B"

    st.info("ğŸš€ ëª¨ë¸ì„ ë¡œë“œ ì¤‘ì…ë‹ˆë‹¤... ì²« ì‹¤í–‰ì€ 1-2ë¶„ ì†Œìš”ë©ë‹ˆë‹¤.")

    base_model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True
    )
    base_tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)

    try:
        from transformers.trainer_utils import get_last_checkpoint
        last_ckpt = get_last_checkpoint(MODEL_PATH)
        lora_model = PeftModel.from_pretrained(base_model, last_ckpt)
        model = lora_model
    except:
        st.warning("âš ï¸ LoRA ë¡œë“œ ì‹¤íŒ¨, ë² ì´ìŠ¤ ëª¨ë¸ ì‚¬ìš©")
        model = base_model

    model.eval()
    return model, base_tokenizer

# ========================================
# 2. í—¬í¼ í•¨ìˆ˜
# ========================================

def extract_math_answer(text: str) -> str:
    """####ë¡œ êµ¬ë¶„ëœ ìˆ˜í•™ ë‹µ ì¶”ì¶œ"""
    if "####" in text:
        answer_part = text.split("####")[-1].strip()
        numbers = re.findall(r"\d+", answer_part)
        return numbers[0] if numbers else answer_part[:50]
    numbers = re.findall(r"\d+", text)
    return numbers[-1] if numbers else "ë‹µì„ ì°¾ì„ ìˆ˜ ì—†ìŒ"

def generate_response(
    model,
    tokenizer,
    prompt: str,
    max_length: int = 256,
    temperature: float = 0.7,
    top_p: float = 0.9
) -> Tuple[str, float]:
    """ëª¨ë¸ ì¶”ë¡ """

    start_time = time.time()

    chat = [{'role': 'user', 'content': prompt}]
    formatted_prompt = tokenizer.apply_chat_template(
        chat,
        tokenize=False,
        add_generation_prompt=True
    )

    inputs = tokenizer(
        formatted_prompt,
        return_tensors="pt",
        truncation=True,
        max_length=1024
    ).to(model.device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_length,
            temperature=temperature,
            top_p=top_p,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )

    response = tokenizer.decode(
        outputs[0][inputs.input_ids.shape[1]:],
        skip_special_tokens=True
    )

    processing_time = time.time() - start_time
    return response.strip(), processing_time

# ========================================
# 3. Streamlit ë ˆì´ì•„ì›ƒ
# ========================================

st.markdown("""
# ğŸ¤– EXAONE 1.2B LLM ëŒ€ì‹œë³´ë“œ
### LoRA ê²½ëŸ‰í™” ëª¨ë¸ - ì‹¤ì‹œê°„ ì±„íŒ… & ìˆ˜í•™ íŠœí„°
""")

with st.sidebar:
    st.markdown("## âš™ï¸ ì„¤ì •")

    mode = st.radio(
        "ğŸ“Œ ëª¨ë“œ ì„ íƒ",
        ["ì¼ë°˜ ì±„íŒ…", "ìˆ˜í•™ í’€ì´"],
        key="mode_selector"
    )

    st.markdown("---")
    st.markdown("## ğŸ›ï¸ ëª¨ë¸ íŒŒë¼ë¯¸í„°")

    temperature = st.slider(
        "ğŸŒ¡ï¸ Temperature (ì°½ì˜ì„±)",
        0.1, 2.0, 0.7, 0.1,
        help="ë‚®ì„ìˆ˜ë¡ ì¼ê´€ì„± ìˆìŒ, ë†’ì„ìˆ˜ë¡ ì°½ì˜ì "
    )

    top_p = st.slider(
        "ğŸ¯ Top-P (ë‹¤ì–‘ì„±)",
        0.1, 1.0, 0.9, 0.05,
        help="í™•ë¥ ì´ ë†’ì€ í† í°ë§Œ ì„ íƒ"
    )

    max_length = st.slider(
        "ğŸ“ ìµœëŒ€ ê¸¸ì´",
        50, 512, 256, 50,
        help="ìƒì„±í•  í† í°ì˜ ìµœëŒ€ ê°œìˆ˜"
    )

    st.markdown("---")

    st.markdown("## ğŸ“Š ëª¨ë¸ ì •ë³´")
    col1, col2 = st.columns(2)
    with col1:
        st.metric("ëª¨ë¸", "EXAONE 1.2B")
        st.metric("ì–‘ìí™”", "4-bit NF4")
    with col2:
        st.metric("LoRA íŒŒë¼ë¯¸í„°", "1.6M")
        st.metric("íš¨ìœ¨ì„±", "99.1% â†“")

    if torch.cuda.is_available():
        memory_used = torch.cuda.memory_allocated() / 1e9
        memory_total = torch.cuda.get_device_properties(0).total_memory / 1e9
        st.metric("GPU ë©”ëª¨ë¦¬", f"{memory_used:.2f}/{memory_total:.2f} GB")

tab1, tab2, tab3 = st.tabs(["ğŸ’¬ ì±„íŒ…", "ğŸ“ˆ í†µê³„", "ğŸ“š ìƒ˜í”Œ"])

with tab1:
    st.markdown("### ì±—ë´‡ê³¼ ëŒ€í™”í•˜ì„¸ìš”")

    if "messages" not in st.session_state:
        st.session_state.messages = []
    if "model" not in st.session_state:
        st.session_state.model, st.session_state.tokenizer = load_model()
        st.success("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")

    model = st.session_state.model
    tokenizer = st.session_state.tokenizer

    chat_container = st.container()
    with chat_container:
        for msg in st.session_state.messages:
            if msg["role"] == "user":
                st.markdown(f"""
                <div class="message-user">
                    <strong>ğŸ‘¤ You:</strong><br>{msg["content"]}
                </div>
                """, unsafe_allow_html=True)
            else:
                st.markdown(f"""
                <div class="message-assistant">
                    <strong>ğŸ¤– Assistant:</strong><br>{msg["content"]}
                </div>
                """, unsafe_allow_html=True)

    st.markdown("---")
    col1, col2 = st.columns([4, 1])

    with col1:
        user_input = st.text_input(
            "ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”:",
            placeholder="ì˜ˆ: 2 + 2ëŠ” ëª‡ì´ì•¼? / í•œêµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì•¼?",
            key="user_input"
        )

    with col2:
        send_button = st.button("ì „ì†¡", use_container_width=True)

    if send_button and user_input:
        st.session_state.messages.append({"role": "user", "content": user_input})

        with st.spinner("ğŸ¤” ì‘ë‹µ ìƒì„± ì¤‘..."):
            if "ìˆ˜í•™" in mode:
                system_prompt = """ìˆ˜í•™ ë¬¸ì œë¥¼ ë‹¨ê³„ë³„ë¡œ í’€ì–´ì£¼ì„¸ìš”.
ê° ë‹¨ê³„ë¥¼ ëª…í™•íˆ ì„¤ëª…í•˜ê³ ,
ìµœì¢… ë‹µì€ #### ë’¤ì— ìˆ«ìë§Œ ì ì–´ì£¼ì„¸ìš”."""
                full_prompt = f"{system_prompt}\n\nì§ˆë¬¸: {user_input}"
            else:
                full_prompt = user_input

            response, processing_time = generate_response(
                model,
                tokenizer,
                full_prompt,
                max_length=max_length,
                temperature=temperature,
                top_p=top_p
            )

            st.session_state.messages.append({"role": "assistant", "content": response})

            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("ì‘ë‹µ ê¸¸ì´", f"{len(response)} ì")
            with col2:
                st.metric("ì²˜ë¦¬ ì‹œê°„", f"{processing_time:.2f}ì´ˆ")
            with col3:
                if "ìˆ˜í•™" in mode:
                    answer = extract_math_answer(response)
                    st.metric("ì¶”ì¶œëœ ë‹µ", answer)

        st.rerun()

    if not st.session_state.messages:
        st.info("""
        ğŸ‘‹ **EXAONE 1.2Bì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤!**

        - **ì¼ë°˜ ì±„íŒ…**: ììœ ë¡œìš´ ëŒ€í™” (í•œêµ­ì–´)
        - **ìˆ˜í•™ í’€ì´**: ìˆ˜í•™ ë¬¸ì œ ë‹¨ê³„ë³„ í’€ì´

        ê¶ê¸ˆí•œ ì ì„ ì…ë ¥í•´ë³´ì„¸ìš”!
        """)

with tab2:
    st.markdown("### ğŸ“Š ëª¨ë¸ ì„±ëŠ¥ ì§€í‘œ")

    if st.session_state.messages:
        assistant_messages = [m for m in st.session_state.messages if m["role"] == "assistant"]
        user_messages = [m for m in st.session_state.messages if m["role"] == "user"]

        col1, col2, col3, col4 = st.columns(4)

        with col1:
            st.metric("ì´ ëŒ€í™” ìˆ˜", len(st.session_state.messages) // 2)
        with col2:
            avg_response_length = sum(len(m["content"]) for m in assistant_messages) / len(assistant_messages) if assistant_messages else 0
            st.metric("í‰ê·  ì‘ë‹µ ê¸¸ì´", f"{int(avg_response_length)} ì")
        with col3:
            total_chars = sum(len(m["content"]) for m in assistant_messages)
            st.metric("ì´ ìƒì„± ë¬¸ì", f"{total_chars:,} ì")
        with col4:
            st.metric("ì‚¬ìš©ì ì…ë ¥", f"{len(user_messages)}")

        st.markdown("---")

        st.markdown("### ğŸ“ˆ ì‘ë‹µ ê¸¸ì´ ë¶„í¬")
        response_lengths = [len(m["content"]) for m in assistant_messages]

        col1, col2 = st.columns(2)
        with col1:
            st.line_chart(response_lengths, use_container_width=True)
        with col2:
            import statistics
            st.metric("ìµœì†Œ", min(response_lengths))
            st.metric("ìµœëŒ€", max(response_lengths))
            st.metric("í‰ê· ", f"{statistics.mean(response_lengths):.0f}")
            st.metric("ì¤‘ì•™ê°’", f"{statistics.median(response_lengths):.0f}")
    else:
        st.info("ğŸ“Œ ì•„ì§ ë©”ì‹œì§€ê°€ ì—†ìŠµë‹ˆë‹¤. ì±„íŒ… íƒ­ì—ì„œ ì‹œì‘í•´ë³´ì„¸ìš”!")

with tab3:
    st.markdown("### ğŸ“š ì‚¬ìš© ì˜ˆì‹œ")

    st.markdown("#### ğŸ§® ìˆ˜í•™ í’€ì´ ì˜ˆì‹œ")
    sample_math_problems = [
        "ì² ìˆ˜ëŠ” ì‚¬ê³¼ 12ê°œë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. ì˜í¬ì—ê²Œ 5ê°œë¥¼ ì£¼ì—ˆìŠµë‹ˆë‹¤. ì² ìˆ˜ì—ê²Œ ë‚¨ì€ ì‚¬ê³¼ëŠ” ëª‡ ê°œì¸ê°€ìš”?",
        "í•œ ë°˜ì— í•™ìƒì´ 24ëª…ì…ë‹ˆë‹¤. ë‚¨í•™ìƒì´ ì—¬í•™ìƒë³´ë‹¤ 4ëª… ë” ë§ìŠµë‹ˆë‹¤. ì—¬í•™ìƒì€ ëª‡ ëª…ì¸ê°€ìš”?",
        "ìƒìì— ë¹¨ê°„ ê³µ 15ê°œ, íŒŒë€ ê³µ 20ê°œê°€ ìˆìŠµë‹ˆë‹¤. ì „ì²´ ê³µì€ ëª‡ ê°œì¸ê°€ìš”?"
    ]

    for i, problem in enumerate(sample_math_problems, 1):
        if st.button(f"ğŸ“ ì˜ˆì‹œ {i}: {problem[:40]}...", use_container_width=True):
            st.session_state.messages.append({"role": "user", "content": problem})
            st.rerun()

    st.markdown("---")
    st.markdown("#### ğŸ’¬ ì¼ë°˜ ì±„íŒ… ì˜ˆì‹œ")
    sample_chat = [
        "ì•ˆë…•í•˜ì„¸ìš”! ë‹¹ì‹ ì€ ëˆ„êµ¬ì¸ê°€ìš”?",
        "í•œêµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì¸ê°€ìš”?",
        "ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì–´ë•Œìš”?"
    ]

    for i, chat in enumerate(sample_chat, 1):
        if st.button(f"ğŸ’¬ ì˜ˆì‹œ {i}: {chat}", use_container_width=True):
            st.session_state.messages.append({"role": "user", "content": chat})
            st.rerun()

st.markdown("---")
st.markdown("""
### ğŸ¯ ì •ë³´
- **ëª¨ë¸**: EXAONE 4.0 1.2B (LoRA ê²½ëŸ‰í™”)
- **í•™ìŠµ ë°ì´í„°**: GSM8K-Ko (í•œêµ­ì–´ ìˆ˜í•™ ë¬¸ì œ)
- **í”„ë ˆì„ì›Œí¬**: Streamlit + PyTorch + Hugging Face
- **ìµœì í™”**: 4-bit Quantization + LoRA

ğŸ’¡ **íŒ**: ì‚¬ì´ë“œë°”ì—ì„œ ëª¨ë¸ íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì •í•˜ì—¬ ì‘ë‹µ í’ˆì§ˆì„ ê°œì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
""")