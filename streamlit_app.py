import streamlit as st
from transformers import AutoModelForCausalLM, AutoTokenizer
import time
import re
from typing import Tuple

st.set_page_config(
    page_title="EXAONE 1.2B LLM",
    page_icon="ğŸ¤–",
    layout="wide"
)

st.markdown("""
<style>
    .main {
        background: linear-gradient(135deg, #0f172a 0%, #1e293b 100%);
    }
    .stButton > button {
        background: linear-gradient(135deg, #38bdf8 0%, #0284c7 100%);
        color: white;
        border: none;
        border-radius: 8px;
        padding: 10px 24px;
    }
    .message-user {
        background: #38bdf8;
        color: #0f172a;
        padding: 12px 16px;
        border-radius: 8px;
        margin: 8px 0;
        max-width: 70%;
        margin-left: auto;
    }
    .message-assistant {
        background: rgba(56, 189, 248, 0.15);
        border-left: 3px solid #38bdf8;
        padding: 12px 16px;
        border-radius: 8px;
        margin: 8px 0;
    }
</style>
""", unsafe_allow_html=True)

@st.cache_resource
def load_model():
    st.info("ğŸš€ ê²½ëŸ‰í™” ëª¨ë¸ì„ ë¡œë“œ ì¤‘ì…ë‹ˆë‹¤...")
    
    try:
        MODEL_ID = "paddacoco/exaone-1.2b-lora"
        
        st.write(f"ğŸ“¥ ë¡œë”© ì¤‘: {MODEL_ID}")
        
        base_model = AutoModelForCausalLM.from_pretrained(
            MODEL_ID,
            device_map="cpu",
            trust_remote_code=True
        )
        base_tokenizer = AutoTokenizer.from_pretrained(
            MODEL_ID,
            trust_remote_code=True
        )
        
        st.success("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
        return base_model, base_tokenizer
    
    except Exception as e:
        st.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨!")
        st.error(f"ì—ëŸ¬: {str(e)}")
        st.warning("í™•ì¸ ì‚¬í•­:")
        st.write("1. ëª¨ë¸ ë¦¬í¬ê°€ Publicì¸ê°€?")
        st.write("2. ëª¨ë¸ íŒŒì¼ì´ ì™„ì „íˆ ì—…ë¡œë“œëë‚˜?")
        st.write("3. ëª¨ë¸ IDê°€ ë§ë‚˜? (paddacoco/exaone-1.2b-lora)")
        return None, None

def extract_math_answer(text: str) -> str:
    if "####" in text:
        answer_part = text.split("####")[-1].strip()
        numbers = re.findall(r"\d+", answer_part)
        return numbers[0] if numbers else answer_part[:50]
    numbers = re.findall(r"\d+", text)
    return numbers[-1] if numbers else "ë‹µ ì—†ìŒ"

def generate_response(model, tokenizer, prompt: str, max_length: int = 256, temperature: float = 0.7, top_p: float = 0.9) -> Tuple[str, float]:
    if model is None or tokenizer is None:
        return "ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", 0.0
    
    start_time = time.time()
    
    chat = [{'role': 'user', 'content': prompt}]
    formatted_prompt = tokenizer.apply_chat_template(
        chat, tokenize=False, add_generation_prompt=True
    )
    
    inputs = tokenizer(
        formatted_prompt,
        return_tensors="pt",
        truncation=True,
        max_length=1024
    ).to(model.device)
    
    with __import__('torch').no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_length,
            temperature=temperature,
            top_p=top_p,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
    
    response = tokenizer.decode(
        outputs[0][inputs.input_ids.shape[1]:],
        skip_special_tokens=True
    )
    
    processing_time = time.time() - start_time
    return response.strip(), processing_time

st.markdown("""
# ğŸ¤– EXAONE 1.2B LLM ëŒ€ì‹œë³´ë“œ
### LoRA ê²½ëŸ‰í™” ëª¨ë¸
**LG AIMERS 8th Cohort í•´ì»¤í†¤**
""")

with st.sidebar:
    st.markdown("## âš™ï¸ ì„¤ì •")
    mode = st.radio("ğŸ“Œ ëª¨ë“œ", ["ì¼ë°˜ ì±„íŒ…", "ìˆ˜í•™ í’€ì´"], key="mode_selector")
    
    st.markdown("---")
    temperature = st.slider("ğŸŒ¡ï¸ Temperature", 0.1, 2.0, 0.7, 0.1)
    top_p = st.slider("ğŸ¯ Top-P", 0.1, 1.0, 0.9, 0.05)
    max_length = st.slider("ğŸ“ ê¸¸ì´", 50, 512, 256, 50)
    
    st.markdown("---")
    col1, col2 = st.columns(2)
    with col1:
        st.metric("ëª¨ë¸", "EXAONE 1.2B")
    with col2:
        st.metric("ìµœì í™”", "LoRA")

tab1, tab2 = st.tabs(["ğŸ’¬ ì±„íŒ…", "ğŸ“Š í†µê³„"])

with tab1:
    st.markdown("### ì±—ë´‡ê³¼ ëŒ€í™”í•˜ì„¸ìš”")
    
    if "messages" not in st.session_state:
        st.session_state.messages = []
    if "model" not in st.session_state:
        st.session_state.model, st.session_state.tokenizer = load_model()
    
    model = st.session_state.model
    tokenizer = st.session_state.tokenizer
    
    for msg in st.session_state.messages:
        if msg["role"] == "user":
            st.markdown(f'<div class="message-user"><strong>ğŸ‘¤ You:</strong> {msg["content"]}</div>', unsafe_allow_html=True)
        else:
            st.markdown(f'<div class="message-assistant"><strong>ğŸ¤– AI:</strong> {msg["content"]}</div>', unsafe_allow_html=True)
    
    st.markdown("---")
    col1, col2 = st.columns([4, 1])
    
    with col1:
        user_input = st.text_input("ë©”ì‹œì§€:", placeholder="ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”", key="user_input")
    with col2:
        send_button = st.button("ì „ì†¡", use_container_width=True)
    
    if send_button and user_input:
        st.session_state.messages.append({"role": "user", "content": user_input})
        
        with st.spinner("ì‘ë‹µ ìƒì„± ì¤‘..."):
            if "ìˆ˜í•™" in mode:
                system_prompt = "ìˆ˜í•™ ë¬¸ì œë¥¼ ë‹¨ê³„ë³„ë¡œ í’€ì–´ì£¼ì„¸ìš”.\nìµœì¢… ë‹µì€ #### ë’¤ì— ì ì–´ì£¼ì„¸ìš”."
                full_prompt = f"{system_prompt}\n\nì§ˆë¬¸: {user_input}"
            else:
                full_prompt = user_input
            
            response, processing_time = generate_response(
                model, tokenizer, full_prompt, max_length, temperature, top_p
            )
            
            st.session_state.messages.append({"role": "assistant", "content": response})
            
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("ê¸¸ì´", f"{len(response)} ì")
            with col2:
                st.metric("ì‹œê°„", f"{processing_time:.2f}ì´ˆ")
            with col3:
                if "ìˆ˜í•™" in mode:
                    answer = extract_math_answer(response)
                    st.metric("ë‹µ", answer)
        
        st.rerun()
    
    if not st.session_state.messages:
        st.info("ğŸ‘‹ EXAONE 1.2Bì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤!")

with tab2:
    st.markdown("### ğŸ“Š ëª¨ë¸ ì„±ëŠ¥")
    
    if st.session_state.messages:
        assistant_msgs = [m for m in st.session_state.messages if m["role"] == "assistant"]
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("ëŒ€í™” ìˆ˜", len(st.session_state.messages) // 2)
        with col2:
            avg = sum(len(m["content"]) for m in assistant_msgs) / len(assistant_msgs) if assistant_msgs else 0
            st.metric("í‰ê· ", f"{int(avg)} ì")
        with col3:
            total = sum(len(m["content"]) for m in assistant_msgs)
            st.metric("ì´ ìƒì„±", f"{total:,} ì")

st.markdown("---")
st.markdown("**EXAONE 1.2B - LG AIMERS 8th Cohort í•´ì»¤í†¤**")
